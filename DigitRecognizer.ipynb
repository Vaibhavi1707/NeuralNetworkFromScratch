{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nimport sklearn.metrics as metrics\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-24T17:10:07.986444Z","iopub.execute_input":"2021-12-24T17:10:07.987151Z","iopub.status.idle":"2021-12-24T17:10:07.994917Z","shell.execute_reply.started":"2021-12-24T17:10:07.987105Z","shell.execute_reply":"2021-12-24T17:10:07.994183Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"TRAIN_CSV = '/kaggle/input/digit-recognizer/train.csv'\ntrain_df = pd.read_csv(TRAIN_CSV)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:07.997387Z","iopub.execute_input":"2021-12-24T17:10:07.997844Z","iopub.status.idle":"2021-12-24T17:10:10.205808Z","shell.execute_reply.started":"2021-12-24T17:10:07.997813Z","shell.execute_reply":"2021-12-24T17:10:10.204888Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.207842Z","iopub.execute_input":"2021-12-24T17:10:10.208161Z","iopub.status.idle":"2021-12-24T17:10:10.258505Z","shell.execute_reply.started":"2021-12-24T17:10:10.208121Z","shell.execute_reply":"2021-12-24T17:10:10.257494Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Hence, there are no none values","metadata":{}},{"cell_type":"code","source":"countNonInts = 0\nfor dtype in train_df.dtypes:\n    if dtype != int:\n        countNonInts += 1\n\nprint(\"No. of non int columns:\", countNonInts)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.260080Z","iopub.execute_input":"2021-12-24T17:10:10.260604Z","iopub.status.idle":"2021-12-24T17:10:10.266847Z","shell.execute_reply.started":"2021-12-24T17:10:10.260567Z","shell.execute_reply":"2021-12-24T17:10:10.265916Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Hence, no preprocessing is required.","metadata":{}},{"cell_type":"code","source":"features = train_df.drop('label', axis = 1)[:1000]\nlabels = train_df['label'][:1000]","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.268328Z","iopub.execute_input":"2021-12-24T17:10:10.268589Z","iopub.status.idle":"2021-12-24T17:10:10.361921Z","shell.execute_reply.started":"2021-12-24T17:10:10.268559Z","shell.execute_reply":"2021-12-24T17:10:10.360878Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.33, random_state = 42, stratify = labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.365146Z","iopub.execute_input":"2021-12-24T17:10:10.365518Z","iopub.status.idle":"2021-12-24T17:10:10.381752Z","shell.execute_reply.started":"2021-12-24T17:10:10.365475Z","shell.execute_reply":"2021-12-24T17:10:10.381032Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"x_train = np.array(x_train)\ny_train = np.array(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.383040Z","iopub.execute_input":"2021-12-24T17:10:10.383385Z","iopub.status.idle":"2021-12-24T17:10:10.388588Z","shell.execute_reply.started":"2021-12-24T17:10:10.383347Z","shell.execute_reply":"2021-12-24T17:10:10.387517Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from math import exp\nfrom decimal import Decimal\n\ndef relu(x):\n    return np.maximum(x, 0)\n\ndef softmax(x):\n#     print(x)\n    return np.exp(x) / (np.sum(np.exp(x), axis=0))\n\ndef linear(x, m, c):\n    return np.array([m * x[i] + c for i in range(len(x))])\n\ndef derivative_relu(x):\n    return int(x > 0)\n\ndef derivative_softmax(x):\n    s = softmax(x)\n    return [[s[i] * (int(i == j) - s[j]) for j in range(len(x))] for i in range(len(x))]\n\ndef derivative_linear(m):\n    return m ","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.390589Z","iopub.execute_input":"2021-12-24T17:10:10.390907Z","iopub.status.idle":"2021-12-24T17:10:10.400267Z","shell.execute_reply.started":"2021-12-24T17:10:10.390868Z","shell.execute_reply":"2021-12-24T17:10:10.399693Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from math import log\n\nclass NeuralNetworkClassifier:\n    BIAS = 1\n    ACTIVATION = {'relu': relu, 'softmax': softmax, 'linear': linear}\n    DERIVATIVE = {'relu': derivative_relu, 'softmax': derivative_softmax, \n                  'linear': derivative_linear}\n    CORRECTION = 1e-6\n    \n    def __init__(self, layers):\n        self.layers = layers\n        self.L = len(layers)\n        \n        self.lin_coeffs = {}\n        prev_next = 0\n        for l, layer in enumerate(layers):\n            prev, next, activation = layer\n            \n            if l > 0 and prev_next != prev:\n                raise Exception(\"The no. of layers in this layer is inconsistent.\"\n                                + \"\\nThe last layer mentioned no. of layers in \" \n                                + str(l) + \"th layer is \" + str(prev_next) \n                                + \".\\nBut this layer mentions the no. of layers to be \" \n                                + str(prev) + \".\")\n            \n            if activation == 'linear':\n                self.lin_coeffs[l] = (np.random.rand(), np.random.rand())\n                \n            prev_next = next            \n\n        self.wts = [np.random.randn(next, prev + 1) for prev, next, activation in layers[:-1]]\n        self.wts.append(np.random.randn(layers[-1][1], layers[-1][0] + 1))\n        self.wts.insert(0, [])\n        \n        self.act = [[] for i in range(self.L + 1)]\n        self.z = [[] for i in range(self.L + 1)]\n\n    def _forward_propogation(self, x):\n        x = np.append(x, self.BIAS)\n        self.act[0] = x\n        \n        for l in range(1, self.L + 1):\n            self.z[l] = self.wts[l] @ self.act[l - 1]\n            activation = self.layers[l - 1][2]\n            \n            if activation == 'linear':\n                m, c = self.lin_coeffs[l]\n                self.act[l] = np.append(self.ACTIVATION[activation](self.z[l], m, c), 1)\n            else:\n                self.act[l] = np.append(self.ACTIVATION[activation](self.z[l]), 1)  \n\n        return self.act[self.L]\n    \n    def _backward_propogation(self, y):\n        for l in range(1, self.L):\n            for i in range(len(self.wts[l])):\n                for j in range(len(self.wts[l][0])):\n                    if i == y:\n                        activation = self.layers[l - 1][2]\n                        \n                        if activation == 'linear':\n                            m = self.lin_coeffs[l][0]\n                            derivative = self.DERIVATIVE[activation](m)\n                        else:\n                            derivative = self.DERIVATIVE[activation](self.z[l][y])\n                             \n                        # print(\"ZERO ERROR\", float(self.act[l][y]))\n                        scalers = self.alpha * self.act[l - 1][j] / float(self.act[l][y] + self.CORRECTION)\n                        \n                        self.wts[l][i][j] -= scalers * derivative \n\n    def fit_once(self, X, Y, alpha):\n        n_cols = X.shape[1]\n        \n        if n_cols != self.layers[0][0]:\n            raise Exception(\"The no. of neurons in the first layer should be the same as the no. of columns in X \" \n                            + \"\\nNo. of columns in X \" + str(n_cols) \n                            + \"\\nNo. of neurons in first layer \" + str(self.layers[0][0]))  \n            \n        if max(Y) + 1 != self.layers[-1][1]:\n            raise Exception(\"The no. of neurons in the last layer should be the same as the no. of classes in Y\"\n                            + \"\\nNo. of classes in Y \" + str(max(Y) + 1)\n                            + \"\\nNo. of neurons in the last layer \" + str(self.layers[-1][1]))      \n        self.alpha = alpha\n        \n        for x, y in list(zip(X, Y)):\n            self._forward_propogation(x)\n            self._backward_propogation(y)\n    \n    def predict(self, x):\n        n_cols = x.shape[1]\n        \n        if n_cols != self.layers[0][0]:\n            raise Exception(\"The no. of neurons in the first layer should be the same as the no. of columns in X \" \n                            + \"\\nNo. of columns in X \" + str(n_cols) \n                            + \"\\nNo. of neurons in first layer \" + str(self.layers[0][0]))  \n            \n        return [self._forward_propogation(x_vec) for x_vec in x]\n    \n    def categorical_cross_entropy_loss(self, y, yhat):\n        if max(y) + 1 != self.layers[-1][1]:\n            raise Exception(\"The no. of neurons in the last layer should be the same as the no. of classes in Y\"\n                            + \"\\nNo. of classes in Y \" + str(max(y) + 1)\n                            + \"\\nNo. of neurons in the last layer \" + str(self.layers[-1][1])) \n        error = 0\n        for i, y_label in enumerate(y):\n            error += -log(yhat[i][y_label] + self.CORRECTION)\n        return error","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:14:30.635483Z","iopub.execute_input":"2021-12-24T17:14:30.635785Z","iopub.status.idle":"2021-12-24T17:14:30.659722Z","shell.execute_reply.started":"2021-12-24T17:14:30.635757Z","shell.execute_reply":"2021-12-24T17:14:30.658767Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"max(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.431645Z","iopub.execute_input":"2021-12-24T17:10:10.432055Z","iopub.status.idle":"2021-12-24T17:10:10.448539Z","shell.execute_reply.started":"2021-12-24T17:10:10.432012Z","shell.execute_reply":"2021-12-24T17:10:10.447421Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n#     print(len(y_true), len(y_pred))\n    return np.sum(y_true == y_pred) / float(len(y_true))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.450066Z","iopub.execute_input":"2021-12-24T17:10:10.450356Z","iopub.status.idle":"2021-12-24T17:10:10.460613Z","shell.execute_reply.started":"2021-12-24T17:10:10.450326Z","shell.execute_reply":"2021-12-24T17:10:10.459898Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"len(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:10:10.462051Z","iopub.execute_input":"2021-12-24T17:10:10.462467Z","iopub.status.idle":"2021-12-24T17:10:10.474858Z","shell.execute_reply.started":"2021-12-24T17:10:10.462403Z","shell.execute_reply":"2021-12-24T17:10:10.473812Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"NUM_COLUMNS_X = 784\nNUM_CLASSES = 10\nmodel = NeuralNetworkClassifier([(NUM_COLUMNS_X, 1000, \"relu\"), (1000, NUM_CLASSES, \"softmax\")])\n\nlosses = []\nx_test = np.array(x_test)\ny_test = np.array(y_test)\nNUM_ITERS = 200\nfor _ in range(NUM_ITERS):\n    yhat = model.predict(x_train)\n    print(y_train.shape, len(yhat))\n    loss = model.categorical_cross_entropy_loss(y_train, yhat)\n    print(\"Curr loss:\", loss)\n#     print(x_test[0].shape)\n    test_preds = [np.argmax(preds) for preds in model.predict(x_test)]\n    test_accuracy = accuracy(y_test, test_preds)\n    print(\" Test Accuracy: \", test_accuracy)\n    train_preds = [np.argmax(preds) for preds in yhat]\n    train_accuracy = accuracy(y_train, train_preds)\n    print(\" Train Accuracy: \", train_accuracy)\n    losses.append(loss)\n    print(\"List of losses:\", losses)\n    model.fit_once(x_train, y_train, 0.1)\n    print(\"Iter no:\", _)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T17:14:33.221255Z","iopub.execute_input":"2021-12-24T17:14:33.221960Z","iopub.status.idle":"2021-12-24T17:14:43.010599Z","shell.execute_reply.started":"2021-12-24T17:14:33.221915Z","shell.execute_reply":"2021-12-24T17:14:43.009369Z"},"trusted":true},"execution_count":39,"outputs":[]}]}
